{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "import re\n",
    "import math\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "header = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/65.0.3325.220 Whale/1.3.51.7 Safari/537.36',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword = '인공지능 문제'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url= 'https://news.joins.com/Search/TotalNews?page=%d&Keyword=%s&PeriodType=DirectInput&StartSearchDate=2016.01.01&EndSearchDate=2020.12.31&SortType=New&SearchCategoryType=TotalNews'% (1,keyword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = requests.get(url, headers=header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 페이지 수 확인\n",
    "soup=BeautifulSoup(res.content, 'html.parser')\n",
    "\n",
    "page_base=soup.find(\"span\",{'class':'total_number'}).text\n",
    "page_base1=re.split('\\/',page_base)[0]\n",
    "\n",
    "page_end=int(re.split('-', page_base1)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### 기사 링크 수집\n",
    "\n",
    "href_all=[]\n",
    "check_list=[]#가져오지 못한 페이지 있는지 확인하기 위해 만든 리스트\n",
    "\n",
    "for page in range(1,page_end+1) :\n",
    "    print('page :' + str(page) + '/' + str(page_end))\n",
    "    \n",
    "    url= 'https://news.joins.com/Search/TotalNews?page=%d&Keyword=%s&PeriodType=DirectInput&StartSearchDate=2016.01.01&EndSearchDate=2020.12.31&SortType=New&SearchCategoryType=TotalNews'% (page,keyword)\n",
    "    \n",
    "    res = requests.get(url, headers=header)  \n",
    "    \n",
    "    #데이터를 잘 가져오고 있는지 확인\n",
    "    \n",
    "    if res.status_code==200:\n",
    "        check_list.append('정상')\n",
    "    else:\n",
    "        check_list.append('에러')\n",
    "        continue\n",
    "        \n",
    "    ### 2페이지 이상부터 페이지 정보 안가져와졌으면 다시 가져오기\n",
    "    \n",
    "    if page>1:\n",
    "        if soup==BeautifulSoup(res.content, 'html.parser'):\n",
    "            while True:\n",
    "                res = requests.get(url, headers=header)\n",
    "                if soup!=BeautifulSoup(res.content, 'html.parser'):\n",
    "                    break\n",
    "    \n",
    "    \n",
    "    ### 기사 링크 수집\n",
    "    \n",
    "    soup=BeautifulSoup(res.content, 'html.parser')\n",
    "    \n",
    "    href = soup.find_all(\"h2\",{'class':'headline mg'})\n",
    "   \n",
    "    for i in href:\n",
    "        href_all.append(i.find('a')['href'])\n",
    "    \n",
    "    ### 페이지의 데이터를 전부 가져오지 못했으면 다시 가져오기\n",
    "    \n",
    "    if page!=page_end:\n",
    "        if len(href) !=10:\n",
    "            while True:\n",
    "                res = requests.get(url, headers=header)  \n",
    "                soup==BeautifulSoup(res.content, 'html.parser')\n",
    "                res = requests.get(url, headers=header)  \n",
    "                \n",
    "                soup=BeautifulSoup(res.content, 'html.parser')\n",
    "    \n",
    "                href = soup.find_all(\"h2\",{'class':'headline mg'})\n",
    "\n",
    "                for i in href:\n",
    "                    href_all.append(i.find('a')['href'])\n",
    "                    \n",
    "                if len(href)==10:\n",
    "                    break\n",
    "    \n",
    "    href=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_slice(date_base):\n",
    "    date_base1=re.sub('[^0-9]','',date_base)\n",
    "    year=date_base1[0:4]\n",
    "    month = date_base1[4:6]\n",
    "    day = date_base1[6:8]\n",
    "    return(year,month,day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "check_list_2=[]\n",
    "data_all = pd.DataFrame()\n",
    "page=0\n",
    "\n",
    "for url in href_all :\n",
    "    \n",
    "    page= page+1\n",
    "    print('page :' + str(page) + '/' + str(len(href_all)))\n",
    "    \n",
    "    res = requests.get(url, headers=header)  \n",
    "    \n",
    "    if res.status_code==200:\n",
    "        check_list_2.append('정상')\n",
    "    else:\n",
    "        check_list_2.append('에러')\n",
    "        continue\n",
    "        \n",
    "\n",
    "    if page>1:\n",
    "        if soup==BeautifulSoup(res.content, 'html.parser'):\n",
    "            while True:\n",
    "                res = requests.get(url, headers=header)\n",
    "                if soup!=BeautifulSoup(res.content, 'html.parser'):\n",
    "                    break\n",
    "    \n",
    "    \n",
    "    \n",
    "    soup=BeautifulSoup(res.content, 'html.parser')\n",
    "    \n",
    "    title = soup.select_one('div > h1').text\n",
    "    \n",
    "    try:\n",
    "        content=soup.find('div',{'class':'article_body mg fs4'}).text\n",
    "    except:\n",
    "        content = soup.find('div',{'class':'article_body fs1 mg'}).text\n",
    "    \n",
    "    date_base=soup.find('div',{'class':'byline'}).text\n",
    "    year=date_slice(date_base)[0]\n",
    "    month=date_slice(date_base)[1]\n",
    "    day=date_slice(date_base)[2]\n",
    "    \n",
    "    data = {\"Title\":[title], \"Content\" : [content], \"Year\" :[year], \"Month\" : [month], \"Day\" : [day]}\n",
    "    data = pd.DataFrame(data)\n",
    "    \n",
    "    data_all = pd.concat([data_all,data], axis=0)\n",
    "    \n",
    "data_all=data_all.reset_index().drop(['index'],axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in check_list_2:\n",
    "    print(i)\n",
    "check_list_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#불용어 처리\n",
    "df['Title'] = df['Title'].str.replace(r'\\[([^)]+)\\]','') \n",
    "df['Title'] = df['Title'].str.replace('‘', '')\n",
    "df['Title'] = df['Title'].str.replace('’', '')\n",
    "df['Title'] = df['Title'].str.replace(',', '')\n",
    "df['Title'] = df['Title'].str.replace('[', '')\n",
    "df['Title'] = df['Title'].str.replace(']', '')\n",
    "df['Title'] = df['Title'].str.replace('…', '')\n",
    "df['Title'] = df['Title'].str.replace('-', '')\n",
    "df['Title'] = df['Title'].str.replace('·', '')\n",
    "df['Title'] = df['Title'].str.replace('“', '')\n",
    "df['Title'] = df['Title'].str.replace('”', '')\n",
    "df['Title'] = df['Title'].str.replace('\"', '')\n",
    "df['Title'] = df['Title'].str.replace('···', '')\n",
    "df['Title'] = df['Title'].str.replace(\"'\", '')\n",
    "df['Title'] = df['Title'].str.replace('?', '')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Content'] = df['Content'].str.replace(r'\\(([^)]+)\\)','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#불용어 처리\n",
    "df['Content'] = df['Content'].str.replace('[', '')\n",
    "df['Content'] = df['Content'].str.replace(']', '')\n",
    "df['Content'] = df['Content'].str.replace(\"'\", \"\")  \n",
    "df['Content'] = df['Content'].str.replace('\"', '')\n",
    "df['Content'] = df['Content'].str.replace(',', '')\n",
    "df['Content'] = df['Content'].str.replace(r'\\\\n', '') \n",
    "df['Content'] = df['Content'].str.replace(r'\\\\r', '')\n",
    "df['Content'] = df['Content'].str.replace(r'\\\\xa0', '')\n",
    "df['Content'] = df['Content'].str.replace('‘', '')\n",
    "df['Content'] = df['Content'].str.replace('’', '')\n",
    "df['Content'] = df['Content'].str.replace('“', '')\n",
    "df['Content'] = df['Content'].str.replace('”', '')\n",
    "df['Content'] = df['Content'].str.replace('.', '')\n",
    "df['Content'] = df['Content'].str.replace('//', '')\n",
    "df['Content'] = df['Content'].str.replace('ㆍ', '')\n",
    "df['Content'] = df['Content'].str.replace('  ', '')\n",
    "df['Content'] = df['Content'].str.replace('·', '')\n",
    "df['Content'] = df['Content'].str.replace('…', '')\n",
    "df['Content'] = df['Content'].str.replace('〈br〉', '')\n",
    "df['Content'] = df['Content'].str.replace('「', '')\n",
    "df['Content'] = df['Content'].str.replace('」', '')\n",
    "df['Content'] = df['Content'].str.replace(r'\\\\', '')\n",
    "df['Content'] = df['Content'].str.replace(r'/*', '')\n",
    "df['Content'] = df['Content'].str.replace('◇','')\n",
    "df['Content'] = df['Content'].str.replace('＜','')\n",
    "df['Content'] = df['Content'].str.replace('＞','')\n",
    "df['Content'] = df['Content'].str.replace('△','')\n",
    "df['Content'] = df['Content'].str.replace('▲','')\n",
    "df['Content'] = df['Content'].str.replace('=','')\n",
    "df['Content'] = df['Content'].str.replace('||','')\n",
    "df['Content'] = df['Content'].str.replace('{','')\n",
    "df['Content'] = df['Content'].str.replace('}','')\n",
    "df['Content'] = df['Content'].str.replace('()','')\n",
    "df['Content'] = df['Content'].str.replace(';','')\n",
    "df['Content'] = df['Content'].str.replace(')','')\n",
    "df['Content'] = df['Content'].str.replace('(','')\n",
    "df['Content'] = df['Content'].str.replace('*','')\n",
    "df['Content'] = df['Content'].str.replace('＜1-2＞','')\n",
    "df['Content'] = df['Content'].str.replace('\\s{2,}','') \n",
    "df['Content'] = df['Content'].str.replace('#\\S+','') #delete#tag  \n",
    "df['Content'] = df['Content'].str.replace('@\\S+','') #@xx제거\n",
    "df['Content'] = df['Content'].str.replace('주요기사.*','') \n",
    "df['Content'] = df['Content'].str.replace('©','') \n",
    "df['Content'] = df['Content'].str.replace('○',' ')\n",
    "df['Content'] = df['Content'].str.replace('■','')\n",
    "df['Content'] = df['Content'].str.replace('●','')\n",
    "df['Content'] = df['Content'].str.replace('▽','')\n",
    "df['Content'] = df['Content'].str.replace('《','')\n",
    "df['Content'] = df['Content'].str.replace('》','')\n",
    "df['Content'] = df['Content'].str.replace('〃','')\n",
    "df['Content'] = df['Content'].str.replace(r'ufeff','') \n",
    "df['Content'] = df['Content'].str.replace('사진 게티이미지','')\n",
    "\n",
    "df['Content'] = df['Content'].str.lstrip() #공백제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.insert(5, \"URL\",href_url, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nows = time.localtime()\n",
    "times = str(nows.tm_year) + '-' + str(nows.tm_mon) + '-' + str(nows.tm_mday)\n",
    "name = times + '중앙.xlsx'\n",
    "\n",
    "writer = pd.ExcelWriter(name, engine='xlsxwriter', options={'strings_to_urls': True})\n",
    "df.to_excel(writer)\n",
    "writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
