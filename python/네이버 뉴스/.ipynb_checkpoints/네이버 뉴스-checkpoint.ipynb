{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "import requests\n",
    "import pandas as pd\n",
    "import re\n",
    "import math\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "import time\n",
    "import json\n",
    "import os, sys\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "header = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/65.0.3325.220 Whale/1.3.51.7 Safari/537.36',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def today_1():\n",
    "    year = time.localtime()[0]\n",
    "    month = time.localtime()[1]\n",
    "    day = time.localtime()[2]\n",
    "    \n",
    "    if len(str(month)) == 1:\n",
    "        month = '0' + str(time.localtime()[1])\n",
    "    \n",
    "    return(str(year)+str(month)+str(day))\n",
    "\n",
    "def today_2():\n",
    "    year = time.localtime()[0]\n",
    "    month = time.localtime()[1]\n",
    "    day = time.localtime()[2]\n",
    "    \n",
    "    if len(str(month)) == 1:\n",
    "        month = '0' + str(time.localtime()[1])\n",
    "    \n",
    "    return(str(year)+'-'+str(month)+'-'+str(day))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#속보의 정치 / 생활,문화 / 세계 / IT,과학 / TV연예 / 스포츠"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_list = ['속보의 정치','생활,문화','세계','IT,과학','TV연예','야구','해외야구','축구','해외축구','농구','배구','골프','일반','e스포츠&게임']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_list  = ['https://news.naver.com/main/list.nhn?mode=LSD&mid=sec&sid1=100&date=20210426&page=1',\n",
    "            'https://news.naver.com/main/list.nhn?mode=LSD&mid=sec&sid1=103&date=20210426&page=1',\n",
    "            'https://news.naver.com/main/list.nhn?mode=LSD&mid=sec&sid1=104&date=20210426&page=1',\n",
    "            'https://news.naver.com/main/main.nhn?mode=LSD&mid=shm&sid1=105&date=20210426&page=1',\n",
    "            'https://entertain.naver.com/now#sid=106&date=2021-04-21&page=1',\n",
    "             'https://sports.news.naver.com/kbaseball/news/index.nhn?isphoto=N&date=20210421&page=1',\n",
    "             'https://sports.news.naver.com/wbaseball/news/index.nhn?isphoto=N&date=20210421&page=1',\n",
    "             'https://sports.news.naver.com/kfootball/news/index.nhn?isphoto=N&date=20210421&page=1',\n",
    "             'https://sports.news.naver.com/wfootball/news/index.nhn?isphoto=N&date=20210421&page=1',\n",
    "             'https://sports.news.naver.com/basketball/news/index.nhn?&date=20210421&isphoto=N&page=1',\n",
    "             'https://sports.news.naver.com/volleyball/news/index.nhn?isphoto=N&date=20210421&page=1',\n",
    "             'https://sports.news.naver.com/golf/news/index.nhn?isphoto=N&date=20210421&page=1',\n",
    "             'https://sports.news.naver.com/general/news/index.nhn?isphoto=N&date=20210421&page=1',\n",
    "             'https://sports.news.naver.com/esports/news/index.nhn?isphoto=N&date=20210421&page=1'\n",
    "             \n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_list1  = ['https://news.naver.com/main/list.nhn?mode=LSD&mid=sec&sid1=100&listType=title',\n",
    "            'https://news.naver.com/main/list.nhn?mode=LSD&mid=sec&sid1=103&listType=title',\n",
    "            'https://news.naver.com/main/list.nhn?mode=LSD&mid=sec&sid1=104&listType=title',\n",
    "            'https://news.naver.com/main/list.nhn?mode=LSD&mid=sec&sid1=105&listType=title',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_list2 = 'https://entertain.naver.com/now#sid=106&date=2021-04-21&page=1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "category_num = -1\n",
    "for url in url_list1:\n",
    "    \n",
    "    category_num = category_num+1\n",
    "    \n",
    "    title_all = []\n",
    "    url_all=[]\n",
    "    check_list=[]\n",
    "    page= 0 \n",
    "    while True:\n",
    "        page= page+1\n",
    "        print(name_list[category_num]+':'+(str(page)))\n",
    "        \n",
    "        today = today_1()\n",
    "        \n",
    "        url_date = url+'&date=%s&page=%s' % (today,page)\n",
    "        \n",
    "        res = requests.get(url_date, headers=header)\n",
    "        \n",
    "        if res.status_code==200:\n",
    "            check_list.append('정상')\n",
    "        else:\n",
    "            check_list.append('에러')\n",
    "            continue\n",
    "        \n",
    "        ### 2페이지 이상부터 페이지 정보 안가져와졌으면 다시 가져오기\n",
    "        \n",
    "        if page>1:\n",
    "            if soup==BeautifulSoup(res.content, 'html.parser'):\n",
    "                while True:\n",
    "                    res = requests.get(url_date, headers=header)\n",
    "                    if soup!=BeautifulSoup(res.content, 'html.parser'):\n",
    "                        break\n",
    "        \n",
    "        ### 기사 제목, 링크 수집\n",
    "        \n",
    "        soup=BeautifulSoup(res.content, 'html.parser')\n",
    "        \n",
    "        base=soup.find('div',{'class':'list_body newsflash_body'} )\n",
    "        base=base.find_all('a',{'class':'nclicks(fls.list)'})\n",
    "        \n",
    "        for i in base:\n",
    "            title_all.append(i.text)\n",
    "            url_all.append(i['href'])\n",
    "        \n",
    "        if len (base)!=50 :\n",
    "            break\n",
    "            \n",
    "    df = {\"Title\":title_all, \"URL\" : url_all}\n",
    "    df = pd.DataFrame(df)\n",
    "\n",
    "    nows = time.localtime()\n",
    "    times = str(nows.tm_year) + '-' + str(nows.tm_mon) + '-' + str(nows.tm_mday)\n",
    "    name = times + name_list[category_num] +'.xlsx'\n",
    "\n",
    "    writer = pd.ExcelWriter(name, engine='xlsxwriter', options={'strings_to_urls': True})\n",
    "    df.to_excel(writer)\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = webdriver.ChromeOptions()\n",
    "# options.headless = True\n",
    "options.add_argument(\"window-size=1920x1080\")\n",
    "\n",
    "if getattr(sys, 'frozen', False):\n",
    "    chromedriver_path = os.path.join(sys._MEIPASS, \"chromedriver.exe\")\n",
    "    driver = webdriver.Chrome(chromedriver_path,options=options)\n",
    "else:\n",
    "    driver = webdriver.Chrome(options=options)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_all = []\n",
    "url_all=[]\n",
    "check_list=[]\n",
    "\n",
    "page= 0 \n",
    "while True:\n",
    "    page= page+1\n",
    "    print(name_list[4]+':'+(str(page)))\n",
    "\n",
    "    today = today_2()\n",
    "\n",
    "    url_date = 'https://entertain.naver.com/now#sid=106&date=%s&page=%s' %(today, page)\n",
    "\n",
    "    driver.get(url_date)\n",
    "    \n",
    "    time.sleep(1)\n",
    "\n",
    "   \n",
    "\n",
    "    ### 2페이지 이상부터 페이지 정보 안가져와졌으면 다시 가져오기\n",
    "\n",
    "    if page>1:\n",
    "        if soup==driver.page_source:\n",
    "            break\n",
    "    ### 기사 제목, 링크 수집\n",
    "\n",
    "    soup = driver.page_source\n",
    "\n",
    "    ### 기사 제목, 링크 수집\n",
    "\n",
    "    base=driver.find_elements_by_xpath('//*[@id=\"newsWrp\"]/ul/li[*]/div/a')\n",
    "    \n",
    "    \n",
    "\n",
    "    for i in base:\n",
    "        title_all.append(i.text)\n",
    "        url_all.append(i.get_attribute('href'))\n",
    "\n",
    "    if len(base)!= 25: \n",
    "        break\n",
    "\n",
    "df = {\"Title\":title_all, \"URL\" : url_all}\n",
    "df = pd.DataFrame(df)\n",
    "\n",
    "nows = time.localtime()\n",
    "times = str(nows.tm_year) + '-' + str(nows.tm_mon) + '-' + str(nows.tm_mday)\n",
    "name = times + name_list[4] +'.xlsx'\n",
    "\n",
    "writer = pd.ExcelWriter(name, engine='xlsxwriter', options={'strings_to_urls': True})\n",
    "df.to_excel(writer)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "url_list2 =[             'https://sports.news.naver.com/kbaseball/news/index.nhn?isphoto=N',\n",
    "             'https://sports.news.naver.com/wbaseball/news/index.nhn?isphoto=N',\n",
    "             'https://sports.news.naver.com/kfootball/news/index.nhn?isphoto=N',\n",
    "             'https://sports.news.naver.com/wfootball/news/index.nhn?isphoto=N',\n",
    "             'https://sports.news.naver.com/basketball/news/index.nhn?&isphoto=N',\n",
    "             'https://sports.news.naver.com/volleyball/news/index.nhn?isphoto=N',\n",
    "             'https://sports.news.naver.com/golf/news/index.nhn?isphoto=N',\n",
    "             'https://sports.news.naver.com/general/news/index.nhn?isphoto=N',\n",
    "             'https://sports.news.naver.com/esports/news/index.nhn?isphoto=N'\n",
    "             ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page = 0\n",
    "title_all = []\n",
    "url_all=[]\n",
    "check_list=[]\n",
    "\n",
    "for url in url_list2:\n",
    "    \n",
    "    page = 0\n",
    "    while True:\n",
    "        page= page+1\n",
    "        print('스포츠:'+(str(page)))\n",
    "        \n",
    "        today = today_1()\n",
    "        \n",
    "        url_date = url+'&date=%s&page=%s' % (today,page)\n",
    "        \n",
    "        driver.get(url_date)\n",
    "        \n",
    "        time.sleep(1)\n",
    "        \n",
    "        \n",
    "        \n",
    "        ### 2페이지 이상부터 페이지 정보 안가져와졌으면 다시 가져오기\n",
    "        \n",
    "        if page>1:\n",
    "            if soup==driver.page_source:\n",
    "                break\n",
    "        ### 기사 제목, 링크 수집\n",
    "        \n",
    "        soup = driver.page_source\n",
    "        \n",
    "        base=driver.find_elements_by_xpath('//*[@id=\"_newsList\"]/ul/li[*]/div/a')\n",
    "        \n",
    "        \n",
    "        for i in base:\n",
    "            title_all.append(i.text)\n",
    "            url_all.append(i.get_attribute('href'))\n",
    "        \n",
    "        if len (base)!=20 :\n",
    "            break\n",
    "            \n",
    "df = {\"Title\":title_all, \"URL\" : url_all}\n",
    "df = pd.DataFrame(df)\n",
    "\n",
    "nows = time.localtime()\n",
    "times = str(nows.tm_year) + '-' + str(nows.tm_mon) + '-' + str(nows.tm_mday)\n",
    "name = times + '스포츠'+'.xlsx'\n",
    "\n",
    "writer = pd.ExcelWriter(name, engine='xlsxwriter', options={'strings_to_urls': True})\n",
    "df.to_excel(writer)\n",
    "writer.close()\n",
    "\n",
    "driver.quit()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
